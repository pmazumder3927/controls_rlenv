TrajectoryEnv:
  normalize: true
  n_envs: 32
  n_timesteps: !!float 100000
  policy: "MlpPolicy"
  batch_size: 512
  n_steps: 1024
  gamma: 0.999
  learning_rate: !!float 3.6897557411779e-05
  ent_coef: 6.112296542666746e-07
  clip_range: 0.3
  n_epochs: 1
  gae_lambda: 0.9
  max_grad_norm: 0.5
  vf_coef: 0.1573564255774144
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2,
    ortho_init=False,
    activation_fn=nn.ReLU,
    net_arch=dict(pi=[64, 64], vf=[64, 64])
    )"
